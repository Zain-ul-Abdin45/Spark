# Apache PySpark

- Apache Spark is written in Scala Programming Language.PySpark actually is a Python API for Spark.
- It does in-memory computations to analyze data in real-time with batch-processing.



- Installing PySpark in Windows
- For Installing PySpark you'll need Java Development Kit first.
=> Download updated JAVA JDK from https://www.oracle.com/java/technologies/downloads/ according to your requirements.
=> Extract the JDK to C Disk and then add JAVA_PATH to enviroment variables i,e in user variables.
=> Then %JAVA_HOME%\bin to System variables path

=> Download Pyspark from https://www.apache.org/dyn/closer.lua/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz


=> Extract the PySpark to C Disk and then add SPARK_PATH to enviroment variables i,e in user variables.


=> Then %SPARK_HOME%\bin to System variables path

You'll have a ready enviroment You can check by running spark-shell in cmd.
![image](https://user-images.githubusercontent.com/47116254/209581244-9dba3d45-941f-4bf2-810d-798f9cff3272.png)
